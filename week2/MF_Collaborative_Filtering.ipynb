{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d04d962",
   "metadata": {},
   "source": [
    "# <center> Matrix factorization. Collaborative Filtering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b96bbf",
   "metadata": {},
   "source": [
    "<img src='../week1/images/matrix.png' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3b03b",
   "metadata": {},
   "source": [
    "План занятия\n",
    "\n",
    "- Простые user/item-based similarity подходы и статистические бейзлайны\n",
    "- Матричные факторизации на примере SVD\n",
    "- Коллаборативная фильтрация на примере ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e96f5f",
   "metadata": {},
   "source": [
    "### Базовые подходы. Item-based и User-based similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb04657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import scipy.sparse as scs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from numpy.linalg import svd\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10,9)\n",
    "np.set_printoptions(2, suppress=True)\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "# sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcb161",
   "metadata": {},
   "source": [
    "**1. Сделаем первый бейзлайн Top Popular**\n",
    "\n",
    "Популярность можно посчитать по-разному, например, как наибольшее число просмотров по всем пользователям. Сразу хорошо бы понять про допустимость повторов (например, если речь идет про фильмы). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322763ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('critics_reviews.pickle', 'rb') as f:\n",
    "    reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop_movies(reviews):\n",
    "    movies_counts = {}\n",
    "    for user, films in reviews.items():\n",
    "        for film, _ in films.items():\n",
    "            movies_counts.setdefault(film, 0)\n",
    "            movies_counts[film] += 1\n",
    "    movies_counts = dict(sorted(movies_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    return movies_counts\n",
    "\n",
    "\n",
    "def top_pop_recommender(reviews, person):\n",
    "    recoms = get_pop_movies(reviews)\n",
    "    recoms = {k: v for k, v in recoms.items() if k not in reviews[person]}\n",
    "    return recoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pop_recommender(reviews, 'Toby')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2bafb",
   "metadata": {},
   "source": [
    "Если у нас много фильмов, какого параметра не хватает для выдачи рекомендаций?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abbb6eb",
   "metadata": {},
   "source": [
    "**2. Top Popular v.2.0.**\n",
    "\n",
    "Далеко не всегда наиболее часто просматриваемые - это самые лучшие по рейтингу фильмы. Можно посчитать топ фильмов по среднему рейтингу из отзывов пользователей. Сделаем рекомендации по топу популярности на основе средней оценки фильма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b011434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movies_avg_scores(reviews):\n",
    "    movies_rating = {}\n",
    "    for user, films in reviews.items():\n",
    "        for film, score in films.items():\n",
    "            movies_rating.setdefault(film, [])\n",
    "            movies_rating[film].append(score)\n",
    "\n",
    "    movies_rating = {k: np.mean(v) for k, v in movies_rating.items()}\n",
    "    movies_rating = dict(sorted(movies_rating.items(), key=lambda x: x[1], reverse=True))\n",
    "    return movies_rating\n",
    "\n",
    "\n",
    "def top_avg_scores_recommender(reviews, person):\n",
    "    recoms = get_movies_avg_scores(reviews)\n",
    "    new_movies = set(recoms.keys()).difference(reviews[person])\n",
    "    recoms = {k: round(v, 2) for k, v in recoms.items() if k in new_movies}\n",
    "    return recoms\n",
    "\n",
    "top_avg_scores_recommender(reviews, 'Toby')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6ca13",
   "metadata": {},
   "source": [
    "* К тому же, Top popular - это неперсональные рекомендации. Как вы думаете, если повторы допустимы, какие можно сделать персонализированные бейзлайны? При условии, что есть только интеракции (то есть, никакие дополнительные признаки не доступны).\n",
    "\n",
    "Из каких шагов будет состоять имплементация Top Personal с повторами в рекомендациях? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fed3f6",
   "metadata": {},
   "source": [
    "\n",
    "**3. User-based similarity** \n",
    "\n",
    "Простые, но надежные бейзлайны можно посчитать в виде рекомендаций на основе сходства пользователей или объекотв (user/item similarity). \n",
    "\n",
    "Давайте рассмотрим такой вариант - нужно выдать для $i$-ого пользователя  рекомендации на основе похожих на него пользователей. По его соседям соберем множество просмотренных фильмов и выведем топ  рекомендаций в порядке убывания их среднего рейтинга по соседям. Оценка каждого соседа учитывается с учетом веса его схожести с $i$-м пользователем.\n",
    "\n",
    "То есть, шаги следующие:\n",
    "\n",
    "1. Выбор метрики схожести, ее расчет. \n",
    "2. Выбор k соседей для формирования множества из их айтемов.\n",
    "3. Сортировка по убыванию оценок и возвращение top-n рекомендаций пользователю.\n",
    "\n",
    "\n",
    "Какие простые и распространенные метрики схожести (расстояния) можете вспомнить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d648b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_distance(reviews, person1, person2):\n",
    "    sum_of_squares = sum([(reviews[person1][item] - reviews[person2][item])**2\n",
    "                         for item in reviews[person1] if item in reviews[person2]])\n",
    "    return 1/(1 + np.sqrt(sum_of_squares))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0332ce",
   "metadata": {},
   "source": [
    "<img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/2b9c2079a3ffc1aacd36201ea0a3fb2460dc226f'>\n",
    "<img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/b87fab4bd95646a6aa894efe96e894761c94498f'>\n",
    "\n",
    "где $n$ - размер выборки, <br>\n",
    "$x_{i},y_{i}$ - оценки первого и второго пользователя <br>\n",
    "$\\bar x = \\frac{1}{n} \\sum^n_{i=1}x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ca072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_pearson(reviews, person1, person2):\n",
    "    stats = {}\n",
    "    similar = list(set(reviews[person1].keys()) & set(reviews[person2].keys()))\n",
    "    for person in [person1, person2]:\n",
    "        stats.setdefault(person, {'ranks': 0})\n",
    "        stats[person]['ranks'] = [reviews[person][i] for i in similar]\n",
    "        \n",
    "    return np.corrcoef(stats[person1]['ranks'], stats[person2]['ranks'])[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9504e",
   "metadata": {},
   "source": [
    "Для наглядности выведем попарные метрики схожести пользователей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "compared = set()\n",
    "for first in reviews:\n",
    "    for second in reviews:\n",
    "        compared.add((first, second))\n",
    "        if first != second and (second, first) not in compared:\n",
    "            print(f'{first} & {second}: ', round(sim_pearson(reviews, first, second), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579851c5",
   "metadata": {},
   "source": [
    "Давайте посмотрим на коэффициенты корреляции. У нас нашлось несколько примеров, у которых коэффициент корреляции равен 1. И есть даже сильная обратная линейная взаимосвязь: -1.\n",
    "\n",
    "Какие есть недостатки при использовании этой метрики? Как можно использовать пары с отрицательными корреляциями?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f5f6c9",
   "metadata": {},
   "source": [
    "Сортировка самих пользователей на основе схожести:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c407c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_users(reviews, person, n=5, similarity=sim_pearson):\n",
    "    scores = [(other, round(similarity(reviews, person, other),2)) for other in reviews if other != person]\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361442cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_users(reviews, 'Toby', n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf1dc1",
   "metadata": {},
   "source": [
    "Перейдем к самим рекомендациям по найденным похожим пользователям.\n",
    "\n",
    "Кстати, сколько выбирать соседей?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed905494",
   "metadata": {},
   "source": [
    "По всем похожим пользователям (sim>0) берем их рейтинги по всем новым для таргетного пользователя фильмам. Считаем взвешенную сумму эти рейтингов на скор схожести соседа и нормализуем, поделив на сумму скоров схожести всех соседей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e035df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recoms_by_users(prefs, person, similarity=sim_pearson):\n",
    "    totals = {}\n",
    "    sim_sums = {}\n",
    "    for other in prefs:\n",
    "        if other == person:\n",
    "            continue\n",
    "        sim = similarity(prefs, person, other)\n",
    "        if sim <= 0:\n",
    "            continue\n",
    "        for item in prefs[other]:\n",
    "            if item not in prefs[person] or prefs[person][item] == 0:\n",
    "                totals.setdefault(item, 0)\n",
    "                # print(person, other, sim, item, prefs[other][item])\n",
    "                totals[item] += prefs[other][item]*sim\n",
    "                sim_sums.setdefault(item, 0)\n",
    "                sim_sums[item] += sim\n",
    "    rankings = [(item, round(total/sim_sums[item], 2)) for item, total in totals.items()]\n",
    "    rankings = sorted(rankings, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53966611",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recoms_by_users(reviews, 'Toby', sim_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33beb50",
   "metadata": {},
   "source": [
    "**Item-based similarity**\n",
    "\n",
    "Можно посчитать все то же самое, но получить рекомендации пользователя на основе похожести просмотренных им ранее фильмов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e24aeb5",
   "metadata": {},
   "source": [
    "| Метод | Достоинства  |  Недостатки | \n",
    "|---|---|---|\n",
    "| User/item-based similarity | Простота подходов и имплементации|  Для расчетов нужны оценки пользователей (explicit)|  \n",
    "|\n",
    "|| Общая интерпретируемость модели (на основе предпочтений похожих пользователей) | Невозможность предсказаний при холодном старте* |  \n",
    "|\n",
    "|| Хорошее качество для бейзлайна при не сильно разреженных данных | Холодный user/item сам никогда не попадет в рекомендации | \n",
    "|\n",
    "|| Вычислительно незатратно| Ненадеждность корреляций |  \n",
    "|| Можно быстро делать обновления (не зависим от всей выборки)| Popularity bias, поэтому нужно взвешивание, нормализация| \n",
    "|\n",
    "\n",
    "\\* **Cold start problem (проблема холодного старта)** - проблема отсутствия данных по интеракциям у пользователей или айтемов. Неустойчивые к этой проблеме модели тренируются на обучающей выборке и не могут сделать предсказания для новых пользователей и/или айтемов, которые не были включены в train. Например, out-of-sample наблюдения или просто появились новые пользователи/айтемы. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a06ab4",
   "metadata": {},
   "source": [
    "### Что такое колаборативная фильтрация и MF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166f492",
   "metadata": {},
   "source": [
    "**Коллаборативная фильтрация**(CF, Collaborative Filtering) одновременно использует сходство между пользователями и айтемами для рекомендаций. В результате алгоритма мы получает латентные эмбеддинги по пользователям и айтемам исходя из данных матрицы интеракций, в том числе и на implicit данных.\n",
    "\n",
    "Будем исходит из гипотезы, что чем более похожи/релевантны пара $(user_i, item_j)$, тем больше должно быть значение их скалярного произведения эмбеддингов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d17ee7",
   "metadata": {},
   "source": [
    "**Матричная факторизация** (Matrix Factorization, MF) - это матричное разложение, которое по сути является простой латентной эмбеддинг моделью. Будем раскладывать исходную матрицу $A$ на несколько матриц (обычно их 2 или 3), которые будут ее аппроксимировать при перемножении $ PQ^T$. Таким образом:\n",
    "\n",
    "* Матрица интеракций $A \\in R^{m \\times n} $ <br>\n",
    "* Матрица эмбеддингов пользователей $P \\in R^{m \\times dim}$, где ряд $i$ - это эмбеддинг пользователя $i$. <br>\n",
    "* Матрица эмбеддингов айтемов $Q \\in R^{n \\times dim}$, где ряд $j$ - это эмбеддинг айтема $j$.\n",
    "\n",
    "Пара $(i, j)$ в $PQ^T$ - это просто скалярное произведение $<P_i, Q_j>$ эмбедднигов пользователя i и айтема j, которое должно быть близко к $A_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921aa94",
   "metadata": {},
   "source": [
    "**Определение целевой функции для MF**\n",
    "\n",
    "Одной из интуитивно понятных целевых функций для MF explicit данных является квадрат расстояния. Минимизируем по всем парам наблюдаемых записей: \n",
    "\n",
    "$$min_{P \\in R^{m\\times d}, Q \\in R^{n\\times d}} \\sum_{(i,j) \\in obs} (A_{ij} - <P_i, Q_j>)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e51aae",
   "metadata": {},
   "source": [
    "Вы можете решать эту квадратичную задачу с помощью сингулярного разложения матрицы (SVD). Однако SVD тоже не лучшее решение, потому что на практике матрица может быть очень разреженной. Например, подумайте сколько может быть просмотрено конкретным пользователем видео на YouTube из всего их множества. Решение (соответствующее аппроксимации входной матрицы моделью), скорее всего, будет близко к нулю, что тоже приведет к плохой обобщающей способности.\n",
    "\n",
    "**Что же тогда делать?** См. доп. материал в конце ноутбука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random((20, 5))*5\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = svd(A)\n",
    "U.shape, s.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros(A.shape)\n",
    "S[:len(s), :len(s)] = np.diag(s)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ace1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "U @ S @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ebfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(A, U @ S @ V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_k(k: int, U, s, V):\n",
    "    U_new = U[: , :k]\n",
    "    S = np.zeros((U.shape[0], V.shape[1]))\n",
    "    S[:len(s), :len(s)] = np.diag(s)\n",
    "    S_new = S[: k, : k]\n",
    "    V_new = V[: k, :]\n",
    "    return U_new @ S_new @ V_new\n",
    "\n",
    "reshape_for_k(2, U, s, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51189a64",
   "metadata": {},
   "source": [
    "**Сколько компонент брать?**\n",
    "\n",
    "Мы не хотим получить overfit и underfit.\n",
    "\n",
    "Можно построить простые графики метрики качества задачи восстановления исходных данных. Например, **MSE** или **MAE**.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/anamarina/RecSys_course/2021/week2/images/svd3.png' width=500>\n",
    "\n",
    "На третьей компоненте и до пятой величина MSE практически не меняется, по этому остановимся на трёх компонентах.\n",
    "\n",
    "Мы можем добавить коэффициент объясненной дисперсии и разреженность.\n",
    "\n",
    "\n",
    "**Explained variance ratio (EVR)** — коэффициент объясненной дисперсии (оценки информативности компонентов). Эта метрика позволяет вам понять, какой процент общей дисперсии данных объясняется каждой из компонент. Считается так - для каждой компоненты нужно разделить ее собственное или сингулярное значение на сумму всех собственных или сингулярных значений.\n",
    "\n",
    "**Sparsity** — разреженность данных, оценить которую можно отношением количества элементов больше 0 к количеству всех элементов.\n",
    "\n",
    "Как бы вы проинтерпретировали этот график?\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/anamarina/RecSys_course/main/week2/images/svd2.png' height=700 width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaddcce3",
   "metadata": {},
   "source": [
    "Какие еще факторы важны?\n",
    "\n",
    "1. Качество самих рекомендаций (метрики ранжирования на оффлайн и онлайн) данных\n",
    "2. Вычислительные ресурсы (RAM, жетский диск, требования к скорости расчетов и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b1f09",
   "metadata": {},
   "source": [
    "Давайте сначала вспомним и повизуализируем, что значит выбор компонент из SVD на примере картинок и как это влияет на качество восстановления на практике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c475b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nice_pic = np.array(Image.open('../week1/images/neo.png').convert('L'))\n",
    "plt.imshow(nice_pic, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = svd(nice_pic)\n",
    "print((U.shape, s.shape, V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros(nice_pic.shape)\n",
    "S[:len(s), :len(s)] = np.diag(s)\n",
    "print(S.shape)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(U @ S @ V, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_k(k: int, U, s, V):\n",
    "    U_new = U[: , :k]\n",
    "    S = np.zeros((U.shape[0], V.shape[1]))\n",
    "    S[:len(s), :len(s)] = np.diag(s)\n",
    "    S_new = S[: k, : k]\n",
    "    V_new = V[: k, :]\n",
    "    return U_new @ S_new @ V_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = reshape_for_k(15, U, s, V)\n",
    "plt.imshow(new_img, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26021d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(k=widgets.IntSlider(min=1,max=631,step=10,value=10))\n",
    "def plot_figure(k):\n",
    "    plt.title(f'k = {k}')\n",
    "    new_img = reshape_for_k(k, U, s, V)\n",
    "    plt.imshow(new_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81facd",
   "metadata": {},
   "source": [
    "Попробуем применить SVD на табличных данных для задачи рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79504c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ratings.csv')\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_movies = ratings.userId.nunique(), ratings.movieId.nunique()\n",
    "num_users, num_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5373b22c",
   "metadata": {},
   "source": [
    "Разделим выборку на обучение и тест следующим образом: для каждого пользователя в тестовую выборку попадут 10 его последних оценок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(train_ratings.rating);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(train_ratings.groupby('movieId').size(), log_scale=True, height=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratings, test_ratings = [], []\n",
    "num_test_samples = 10\n",
    "\n",
    "for userId, user_data in ratings.groupby('userId'):\n",
    "    train_ratings += [user_data[:-num_test_samples]]\n",
    "    test_ratings += [user_data[-num_test_samples:]]\n",
    "\n",
    "train_ratings = pd.concat(train_ratings)\n",
    "test_ratings = pd.concat(test_ratings)\n",
    "train_ratings.shape, test_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = scs.coo_array((train_ratings.rating, (train_ratings.userId, train_ratings.movieId)), \n",
    "                       shape=(num_users, num_movies)).tocsr()\n",
    "matrix = R.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = svd(matrix)\n",
    "U.shape, s.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros(matrix.shape)\n",
    "S[:len(s), :len(s)] = np.diag(s)\n",
    "print(S.shape)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd54a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_matrix = U @ S @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8251d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(matrix, preds_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "df = pd.DataFrame(data=(zip(matrix[0, :], preds_matrix[0, :])), \n",
    "                  columns=['init', 'preds']).sort_values(by='init', ascending=False)\n",
    "df.sort_values(by='preds', ascending=False).head(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72735fa4",
   "metadata": {},
   "source": [
    "<ins>Вопросы и замечания:</ins> \n",
    "\n",
    "*  1. В чем может быть практическая польза хранить не исходную матрицу, а две/три новые матрицы? \n",
    "\n",
    "*  2. Что важнее - сама оценка скалярного произведения или порядок между оценками?\n",
    "\n",
    "* 3. Подойдет ли нам наиболее точная аппроксимация исходной матрицы для рекомендательных систем (т.е. полностью точно воспроизвели все значения в материце интеракций)? Почему? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438bbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишите метрики качества ранжирования и посчитайте на тестовых данных, насколько хороша SVD для рекомендаций \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847297e",
   "metadata": {},
   "source": [
    "### ALS\n",
    "\n",
    "Итак, поставлена задача построения модели со скрытыми переменными (latent factor model) для коллаборативной фильтрации:\n",
    "\n",
    "$$\\sum_{u,i} (r_{ui} - \\langle p_u, q_i \\rangle)^2 \\to \\min_{P,Q}.\n",
    "\\label{LFM}$$\n",
    "\n",
    "Напомним, что суммирование ведется по всем парам $(u, i),$ для которых известен рейтинг $r_{ui}$ (и только по ним), а $p_u, q_i$ – латентные представления пользователя $u$ и товара $i$ из соответствующих матрицы $P, Q$.\n",
    "\n",
    "Подход ALS (Alternating Least Squares) решает задачу, попеременно фиксируя матрицы $P$ и $Q$, — оказывается, что, зафиксировав одну из матриц, можно выписать аналитическое решение задачи для другой.\n",
    "\n",
    "$$\\nabla_{p_u} \\bigg[ \\sum_{u,i} (r_{ui} - \\langle p_u, q_i \\rangle)^2 \\bigg] = \\sum_{i} 2(r_{ui} - \\langle p_u, q_i \\rangle)q_i = 0$$\n",
    "\n",
    "Воспользовавшись тем, что $a^Tbc = cb^Ta$, получим\n",
    "$$\\sum_{i} r_{ui}q_i - \\sum_i q_i q_i^T p_u = 0.$$\n",
    "Тогда окончательно каждый столбец матрицы $P$ можно найти по формуле\n",
    "$$p_u = \\bigg( \\sum_i q_i q_i^T\\bigg)^{-1}\\sum_ir_{ui}q_i \\;\\; \\forall u,$$\n",
    "аналогично для столбцов матрицы $Q$\n",
    "$$q_i = \\bigg( \\sum_u p_u p_u^T\\bigg)^{-1}\\sum_ur_{ui}p_u \\;\\; \\forall i.$$\n",
    "\n",
    "Таким образом мы можем решать оптимизационную задачу, поочередно фиксируя одну из матриц $P$ или $Q$ и проводя оптимизацию по второй.\n",
    "\n",
    "**Оригинальная статья (для explicit feedback):**\n",
    "\n",
    "Bell, R.M. and Koren, Y., 2007, October. Scalable collaborative filtering with jointly derived neighborhood interpolation weights. In Seventh IEEE international conference on data mining (ICDM 2007) (pp. 43-52). IEEE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9399de",
   "metadata": {},
   "source": [
    "### iALS\n",
    "\n",
    "\n",
    "**Статья для implicit данных:**\n",
    "\n",
    "\n",
    "Implicit feedback ALS:\n",
    "* Hu, Y., Koren, Y. and Volinsky, C., 2008, December. Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE international conference on data mining (pp. 263-272). Ieee. http://yifanhu.net/PUB/cf.pdf\n",
    "\n",
    "**Особенности:**\n",
    "\n",
    "1. Нет explicit данных (явных позитивных и отрицательных оценок). <br>\n",
    "2. Много шума в данных.  <br>\n",
    "3. Используются свои функции и метрики для оценки implicit feedback. Если в explicit $r_ui$ - это оценки предпочтений пользователя, то в implicit - это степень уверенности. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb615e",
   "metadata": {},
   "source": [
    "\n",
    "| Метод | Достоинства  |  Недостатки | \n",
    "|---|---|---|\n",
    "| Collaborative Filtering | Для предсказаний достаточно одних интеракций |  Страдает от проблемы холодного старта|  \n",
    "|\n",
    "|| Serendipity (может выдать для пользователя что-то неожиданно релевантное, создавая интерес к новому) | Низкое качество рекомендаций при маленьком датасете или слишком разреженной матрице |  \n",
    "|\n",
    "|| Общая интерпретируемость модели (на основе предпочтений похожих пользователей) | Не предполагает использование признаков| \n",
    "|\n",
    "||  | User/item никогда не попадет в рекомендации, если нет интеракций|  \n",
    "||  | Popularity bias, поэтому нужно взвешивание, нормализация| \n",
    "|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46936a",
   "metadata": {},
   "source": [
    "Наша задача оптимизации в том, что мы стремимся минимизировать целевую функцию и найти\n",
    "оптимальные $X$ и $Y$. В частности, мы стремимся минимизировать ошибку наименьших квадратов наблюдаемых оценок\n",
    "(и упорядочить):\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/anamarina/RecSys_course/main/week2/images/als_loss.png' width=700 height=500>\n",
    "\n",
    "Для implicit feedback:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/anamarina/RecSys_course/main/week2/images/als_imp.png' width=600 height=600>\n",
    "<img src='https://raw.githubusercontent.com/anamarina/RecSys_course/main/week2/images/als_sol.png' width=500 height=400>\n",
    "\n",
    "\n",
    "Авторы статьи вводят набор переменных, $c_ui$, которые измеряют нашу уверенность в наблюдении за $p_ui$. Чем больше значение наблюдения за пользовательским элементом, тем больше вы уверены в этом значении. Таким образом, у нас есть некоторая минимальная уверенность в $p_ui$ для каждой пары пользовательских элементов, но по мере того, как мы наблюдаем больше положительных предпочтений по паре пользователь-объект, наша уверенность в $p_ui$ = 1 соответственно возрастает. Скорость увеличения регулируется константой $\\alpha$.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/anamarina/RecSys_course/main/week2/images/als.png' width=700 height=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b9dc45",
   "metadata": {},
   "source": [
    "На таких маленьких данных мы можем позволить себе обучить полный ALS с честным обращением матриц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ALS(user_ids, item_ids, ratings, num_users, num_items, num_dims=32, num_iters=10, eps=1e-7):\n",
    "    R = scs.coo_array((ratings, (user_ids, item_ids)), shape=(num_users, num_items)).tocsr()\n",
    "    X = np.random.randn(num_users, num_dims) #U\n",
    "    Y = np.random.randn(num_items, num_dims) #V\n",
    "    \n",
    "    for t in tqdm(range(num_iters)):\n",
    "        RY = R @ Y\n",
    "        for u in range(num_users):\n",
    "            relevant_items = item_ids[user_ids == u]\n",
    "            Y_rel = Y[relevant_items]\n",
    "            YY = Y_rel.reshape(-1, num_dims, 1) * Y_rel.reshape(-1, 1, num_dims)\n",
    "            X[u] = np.linalg.inv(YY.sum(axis=0) + eps * np.eye(num_dims)) @ RY[u]\n",
    "\n",
    "        RX = R.T @ X\n",
    "        for i in range(num_items):\n",
    "            relevant_users = user_ids[item_ids == i]\n",
    "            X_rel = X[relevant_users]\n",
    "            XX = X_rel.reshape(-1, num_dims, 1) * X_rel.reshape(-1, 1, num_dims)\n",
    "            Y[i] = np.linalg.inv(XX.sum(axis=0) + eps * np.eye(num_dims)) @ RX[i]\n",
    "    \n",
    "    return R, X, Y\n",
    "\n",
    "R, X, Y = ALS(train_ratings.userId, train_ratings.movieId, train_ratings.rating, num_users, num_movies, num_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc192fb2",
   "metadata": {},
   "source": [
    "Сгенерируем предсказания, посчитав скалярные произведения между векторами пользователей и фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf32221",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = (X[train_ratings.userId] * Y[train_ratings.movieId]).sum(axis=1)\n",
    "test_preds = (X[test_ratings.userId] * Y[test_ratings.movieId]).sum(axis=1)\n",
    "test_preds = np.clip(test_preds, 0.5, 5.0)\n",
    "print(f'Train R^2: {r2_score(train_ratings.rating, train_preds):.3f}')\n",
    "print(f'Test R^2: {r2_score(test_ratings.rating, test_preds):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc827d4",
   "metadata": {},
   "source": [
    "Кстати, в каких случаях $R^2$ получается отрицательным?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6044cd45",
   "metadata": {},
   "source": [
    "Результаты получились весьма удручающими. Но не будем забывать, что нас интересует рекомендация фильмов, а не предсказание точно рейтинга. Поэтому упорядочим фильмы по предсказанной релевантности (значение скалярного произведения) и посчитаем метрику для ранжирования — MAP@k, где пусть $k=10$. Для сравнения рассмотрим еще два алгоритма — случайный (ранжирует фильмы в произвольном порядке) и алгоритм, возвращающий топ рейтинга фильмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c86ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_relevant = []\n",
    "for user_id, user_data in test_ratings[test_ratings.rating >= 4.0].groupby('userId'):\n",
    "    test_relevant += [user_data.movieId.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5309e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_train_items(preds, k):\n",
    "    new_preds = np.zeros((preds.shape[0], k), dtype=np.int)\n",
    "    for user_id, user_data in train_ratings.groupby('userId'):\n",
    "        user_preds = preds[user_id]\n",
    "        new_preds[user_id] = user_preds[~np.in1d(user_preds, user_data.movieId)][:k]\n",
    "    \n",
    "    return new_preds\n",
    "\n",
    "\n",
    "def ALS_prediction(X, Y, k=10):\n",
    "    preds = np.argsort(X @ Y.T, axis=1)\n",
    "    return preds[:k]\n",
    "\n",
    "\n",
    "def random_prediction(k=10):\n",
    "    preds = np.tile(np.arange(num_movies), (num_users, 1))\n",
    "    for i in range(num_users):\n",
    "        rand_perm = np.random.permutation(num_movies)\n",
    "        preds[i] = preds[i][rand_perm]\n",
    "\n",
    "    return preds[:k]\n",
    "\n",
    "def top_prediction(freq_thr=10, k=10):\n",
    "    mean_rating = train_ratings.groupby('movieId').rating.mean()\n",
    "    mean_rating = mean_rating[train_ratings.groupby('movieId').size() >= freq_thr]\n",
    "    preds = np.array(mean_rating.sort_values(ascending=False).index)\n",
    "    preds = np.tile(preds, (num_users, 1))\n",
    "    return preds[:k]\n",
    "\n",
    "\n",
    "def MAPk(y_true, y_pred, k=10):\n",
    "    map_k, count = 0, 0\n",
    "    for relevant_items, predicted_items in zip(y_true, y_pred):\n",
    "        if not relevant_items:\n",
    "            continue\n",
    "        correct = np.isin(predicted_items[:k], relevant_items)\n",
    "        map_k += (correct / np.arange(1, k + 1)).sum() / \\\n",
    "            (1 / np.arange(1, len(relevant_items) + 1)).sum()\n",
    "        count += 1\n",
    "    \n",
    "    map_k = map_k / count\n",
    "    return map_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d6457",
   "metadata": {},
   "source": [
    "Посчитаем значение MAP@k для разных значений k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1, 51)\n",
    "als_preds = ALS_prediction(X, Y, k=ks[-1])\n",
    "random_preds = random_prediction(k=ks[-1])\n",
    "top_preds = top_prediction(freq_thr=10, k=ks[-1])\n",
    "\n",
    "random_mapk = [MAPk(test_relevant, random_preds, k=k) for k in ks]\n",
    "top_mapk = [MAPk(test_relevant, top_preds, k=k) for k in ks]\n",
    "als_mapk = [MAPk(test_relevant, als_preds, k=k) for k in ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ks, random_mapk, label='Random')\n",
    "plt.plot(ks, top_mapk, label='Top rating')\n",
    "plt.plot(ks, als_mapk, label='ALS')\n",
    "\n",
    "plt.legend(title='method')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('MAP@k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22057aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ks, random_mapk, label='Random')\n",
    "plt.plot(ks, top_mapk, label='Top rating')\n",
    "plt.plot(ks, als_mapk, label='ALS')\n",
    "\n",
    "plt.legend(title='method')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('MAP@k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6a5d2",
   "metadata": {},
   "source": [
    "Померяем также две метрики, которые характеризуют разнообразность предсказаний: coverage — доля фильмов из базы, которые в принципе рекомендуются и personalization — среднее косинусное расстояние между бинарными векторами рекомендаций для разных пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def coverage(preds):\n",
    "    return np.unique(preds.flatten()).size / num_movies\n",
    "\n",
    "\n",
    "def personalization(preds):\n",
    "    N = preds.shape[0]\n",
    "    ohe_matrix = np.zeros((N, num_movies))\n",
    "    ohe_matrix[np.arange(N).repeat(preds.shape[1]), preds.reshape(-1)] = 1.\n",
    "    sim_matrix = cosine_similarity(ohe_matrix)\n",
    "    return 1 - (sim_matrix.sum() - N) / (N * (N - 1))\n",
    "\n",
    "print(f'Random coverage: {coverage(random_preds):.4f}')\n",
    "print(f'Top rating coverage: {coverage(top_preds):.4f}')\n",
    "print(f'ALS coverage: {coverage(als_preds):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c73909",
   "metadata": {},
   "source": [
    "Кроме того, можем попробовать использовать обученные векторы фильмов, чтобы выдавать фильмы, похожие на данные запрос, например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fdabdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_movieId = ratings[ratings.title=='Iron Man'].iloc[0].movieId\n",
    "scores = Y @ Y[example_movieId]\n",
    "similar_movies = np.argsort(-scores)[:20]\n",
    "\n",
    "for movieId in similar_movies:\n",
    "    normalized_score = scores[movieId] / np.linalg.norm(V[example_movieId]) / np.linalg.norm(V[movieId])\n",
    "    print(f'{ratings[ratings.movieId == movieId].iloc[0].title}: {normalized_score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24872aa5",
   "metadata": {},
   "source": [
    "\n",
    "| Критерий | ALS  |  iALS | \n",
    "|---|---|---|\n",
    "| Тип данных | Этот подход используется, когда у вас есть явные рейтинги или оценки, предоставленные пользователями для различных предметов. Например, пользователи могут оценивать фильмы по шкале от 1 до 5. |  Этот подход используется, когда у вас есть неявные сигналы взаимодействия между пользователями и предметами, такие как клики, просмотры, покупки или временные интервалы между действиями. |  \n",
    "|\n",
    "|Цель | Модель стремится точно предсказать явные рейтинги или оценки, которые пользователи могли бы дать предметам. | Модель стремится моделировать уровень уверенности или важности взаимодействия между пользователем и предметом, но не предсказывает явные рейтинги.|  \n",
    "|\n",
    "|Функции потерь| Обычно использует функцию потерь, такую как среднеквадратичная ошибка (MSE), для минимизации разницы между предсказанными и фактическими оценками. | Использует функцию потерь, которая учитывает уверенность в неявных взаимодействиях и стремится увеличить уверенность для более важных взаимодействий.| \n",
    "|\n",
    "|Взвешивание| Не уделяет внимания взаимодействиям, которые не были явно оценены пользователями. Исключает информацию о неявных действиях. | Дополнительная параметризация. Учитывает все неявные действия, но с учетом их уверенности или веса, что позволяет модели учесть важность различных видов взаимодействий| \n",
    "|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd501907",
   "metadata": {},
   "source": [
    "### Доп. материалы\n",
    "\n",
    "Есть **Weighted Matrix Factorization** (взвешенная матричная факторизация), которая раскладывает целевую функцию на две суммы:\n",
    "\n",
    "1) сумма по всем имеющимся интеракциям\n",
    "\n",
    "2) сумма по отсутствующим интеракциям (которые мы обозначали 0)\n",
    "\n",
    "\n",
    "$$min_{P \\in R^{m\\times d}, Q \\in R^{n\\times d}} \\sum_{(i,j) \\in obs} (A_{ij} - <P_i, Q_j>)^2 + w_{0} \\cdot \\sum_{(i,j) \\notin obs} (<P_{i}, Q_{j}>)^2$$\n",
    "\n",
    "Здесь $w_{0}$ — гиперпараметр, который взвешивает две компонентны, чтобы одна не преобладала над другой. Настройка этого гиперпараметра очень важна.\n",
    "\n",
    "На практике, вам также необходимо взвешивать наблюдаемые пары. Например, частые элементы (очень популярные видео на YouTube) или слишком активные пользователи могут доминировать в целевой функции. Вы можете скорректировать этот эффект, взвесив обучающие примеры, чтобы учесть их частоту в датасете. Можно заменить целевую функцию на:\n",
    "\n",
    "$$min_{P \\in R^{m\\times d}, Q \\in R^{n\\times d}} \\sum_{(i,j) \\in obs} w_{i,j} \\cdot (A_{ij} - <P_i, Q_j>)^2 + w_{0} \\cdot \\sum_{(i,j) \\notin obs} (<P_{i}, Q_{j}>)^2$$\n",
    "\n",
    "\n",
    "**Минимизация целевой функции**:\n",
    "\n",
    "* SGD (stochastic gradient descent) - общеизвестный и используемый везде метод, оптимизация происходит по наблюдениям из обучающей выборки. \n",
    "* MCMC (Markov chain Monte Carlo) - сэмплирование параметров из апостериорного распределения. \n",
    "* Weighted Alternating Least Squares (WALS) - специализированный лос под обозначенную выше целевую функцию, с попеременной оптимизацией по каждому параметру при фиксированных значениях остальных параметров. \n",
    "\n",
    "Лосс квадратичный для обеих матриц $P$ и $Q$.  \n",
    "WALS случайным образом инициализирует эмбеддинги и потом последовательно обновляет веса то одной, то второй матрицы:\n",
    "\n",
    "1) Фиксируем $P$, делаем итерацию оптимизации для решения $Q$. \n",
    "\n",
    "2) Фиксируем $Q$, делаем итерацию оптимизации для решения $P$. \n",
    "\n",
    "Каждый этап может быть решен в явном виде (через решение линейной системы). Этот метод гарантированно сходится, потому что каждый шаг гарантированно уменьшает потери.\n",
    "\n",
    "\n",
    "\n",
    "| Метод оптимизации  | Достоинства  |  Недостатки | \n",
    "|---|---|---|\n",
    "|  SGD |  Гибкий в использовании, можно брать другие лоссы | Медленнее, небыстро сходится  |  \n",
    "|  |  Можно распараллелить вычисления | Сложнее работать с отрицательными примерами, нужен negative sampling  |  \n",
    "|  WALS |  Быстрее сходится, чем SGD | Применим только к квадратичным лоссам  | \n",
    "|  |  Легче обрабатывать отсутсвующие интеракции |  |\n",
    "|   |  Можно распараллелить вычисления|  | \n",
    "\n",
    "\n",
    "\n",
    "## SVD \n",
    "\n",
    "<img src='https://www.dataminingapps.com/wp-content/uploads/2020/02/svd2.png'>\n",
    "\n",
    "SVD, помимо скалярного произведения, включает еще смещения по пользователю $b_u$ и объекту $b_i$. Предполагается, что смещения пользователей отражают тенденцию некоторых пользователей оценивать товары выше (или ниже) среднего. То же самое касается товаров: некоторые товары обычно оцениваются выше, чем некоторые другие. Модель SVD тогда выглядит следующим образом:\n",
    "\n",
    "$$\\hat r_{u,i} = \\mu + b_u + b_i + q_{i}^{T}p_{u},$$\n",
    "\n",
    "где $\\mu$ - глобальное среднее по всем оценкам в данных. Тогда задача оптимизации сводится к:\n",
    "\n",
    "$$ \\sum(r_{u,i} - \\hat r_{u,i}) ^ 2 +     \\lambda(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2),$$\n",
    "\n",
    "где $\\lambda$ - параметр регуляризации.\n",
    "\n",
    "Оптимизация через стохастический градиентный спуск:\n",
    "\n",
    "$$b_u \\leftarrow b_u + \\gamma (e_{ui} - \\lambda b_u)$$\n",
    "$$b_i \\leftarrow b_i + \\gamma (e_{ui} - \\lambda b_i)$$  \n",
    "$$p_u \\leftarrow p_u + \\gamma (e_{ui} \\cdot q_i - \\lambda p_u)$$\n",
    "$$q_i \\leftarrow q_i + \\gamma (e_{ui} \\cdot p_u - \\lambda q_i),$$\n",
    "\n",
    "где $\\gamma$ - это learning rate, <br>\n",
    "$e_{ui} =  r_{ui} - \\hat r_{u,i} = r_{u,i} - (\\mu + b_u + b_i + q_{i}^{T}p_{u})$ ошибка алгоритма на паре $(u, i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98733937",
   "metadata": {},
   "source": [
    "### Hierarchical Alternating Least Squares\n",
    "\n",
    "Подход ALS требует вычислений обратных матриц, что накладывает сильные ограничения на размерность скрытых переменных. Альтернативой ALS является подход HALS (Hierarchical Alternating Least Squares), в котором на каждой итерации мы решаем оптимизационную задачу относительно только одной строчки матрицы $P$ или $Q$. Выведем аналитические формулы для $k$-ой строчки матрицы $P$.\n",
    "\n",
    "$$\\frac{\\partial}{\\partial P_{ku}} \\bigg[ \\sum_{u',i} (r_{u'i} - \\langle p_{u'}, q_i \\rangle)^2 \\bigg] = \\sum_{i} 2(r_{ui} - \\langle p_u, q_i \\rangle)Q_{ki} = 0$$\n",
    "\n",
    "$$\\sum_{i} (r_{ui} - \\sum_{s\\neq k} P_{su} Q_{si} )Q_{ki} - P_{ku}\\sum_i Q_{ki}^2 = 0$$\n",
    "\n",
    "$$P_{ku} = \\frac{\\sum_{i} (r_{ui} - \\sum_{s\\neq k} P_{su} Q_{si} )Q_{ki}}{\\sum_i Q_{ki}^2}$$\n",
    "\n",
    "### Neural Collaborative Filtering\n",
    "\n",
    "<img src='https://images.deepai.org/converted-papers/2004.12212/images/NCF.png' width=500>\n",
    "\n",
    "Нелинейным обобщением матричных разложений является коллаборативная фильтрация с помощью нейронных сетей. Сопоставим каждому пользователю $u$ one-hot encode вектор $\\alpha_u$, у которого на $u$-ом месте стоит $1$, а остальные координаты заполнены нулями, аналогично определим вектор $\\beta_i$ для товара $i$. В качестве алгоритма классификации (или регрессии, в зависимости от задачи) будем использовать нейросеть, у которой два полносвязных слоя на входе: один соответствует пользователям и принимает на вход вектор $\\alpha_u$, а второй соответствует товарам и принимает на вход вектор $\\beta_i$. После входных полносвязных слоев соответствующие представления необходимо сконкатенировать и передать в следующие полносвязные слои. На выходе нейронной сети мы получаем предсказание $\\widehat{r}_{ui}$ (на рисунке обозначено как $\\widehat{y}_{ui}$), которое сравниваем с истинным ответом для заданной пары $r_{ui}$ (на рисунке обозначен как $y_{ui}$).\n",
    "\n",
    "Несмотря на свою простоту модель обладает рядом важных достоинств:\n",
    "\n",
    "* мы можем легко обобщить модель на случай, когда у нас есть признаки пользователей или товаров, добавив эти признаки к векторам $\\alpha_u$ и $\\beta_i$ соответственно (также заметим, что добавление признаков в модель позволяет частично решить проблему холодного старта);\n",
    "* в зависимости от размеров выборки и природы данных мы можем адаптировать архитектуру под свои нужды, например, если у нас есть картинки в качестве признаков товаров, мы можем добавить сверточные слои в нашу нейронную сеть и обучать всю модель end-to-end.\n",
    "\n",
    "В этой модели мы по-прежнему можем получить латентные представления пользователей и товаров. На самом деле, латентными представлениями будут столбцы матрицы весов в первых полносвязных слоях (в предположении, что мы умножаем матрицу весов на вектор признаков справа).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a2f17",
   "metadata": {},
   "source": [
    "Советую посмотреть, как считается ALS при работе с дистрибутивными вычислениями (в зависимости от метода: join, broadcast). Так же, есть стриминг ALS. Это когда мы предполагаем, что данные рейтингов $r_ui$ передаются потоком, и что матрицы коэффициентов $X$, $Y$ помещаются в память каждой машины. Мы предполагаем, что поток смешан (shuffled) и мы можем использовать стохастический градиентный спуск(SGD)\n",
    "чтобы обновить матрицы коэффициентов X, Y. \n",
    "<img src='https://raw.githubusercontent.com/anamarina/RecSys_course/main/week2/images/als_st.png' width=500 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4520143",
   "metadata": {},
   "source": [
    "1. https://developers.google.com/machine-learning/recommendation/collaborative/summary \n",
    "2. https://github.com/esokolov/ml-course-hse/blob/master/2021-spring/seminars/sem24-recommendations.pdf\n",
    "3. https://github.com/esokolov/ml-course-hse/blob/master/2021-spring/seminars/sem24-recommendations.ipynb\n",
    "4. https://github.com/esokolov/ml-course-hse/blob/master/2021-spring/homeworks-practice/homework-practice-13-recommendations/homework-practice-13-recommendations.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d066b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

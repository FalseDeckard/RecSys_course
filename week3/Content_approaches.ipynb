{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1db9a5",
   "metadata": {},
   "source": [
    "## <center> Content-based recommender models </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96102d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import islice, cycle, product\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm_notebook\n",
    "from utils.data import MatchDataGenerator, df_to_dict\n",
    "from utils.basic_layers import MLP, EmbeddingLayer\n",
    "from utils.features import SparseFeature, SequenceFeature\n",
    "from utils.match import Annoy, generate_seq_feature_match, gen_model_input\n",
    "from utils.metrics import topk_metrics\n",
    "from utils.trainer import MatchTrainer\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233927c9",
   "metadata": {},
   "source": [
    "**Ключевая идея** - давайте попробуем учиться не только на взаимодействиях users и items, но и добавлять в модель имеющиеся по ним признаки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddca416",
   "metadata": {},
   "source": [
    "Способов немало, но сегодня разберем наиболее интересные и распространенные примеры:\n",
    "- DSSM и ее адаптации\n",
    "- LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834c18b6",
   "metadata": {},
   "source": [
    "#### DSSM (Deep Structured Semantic Model) \n",
    "\n",
    "Вспомним, как выглядела оригинальная модель DSSM для задачи ранжирования выдачи поиска.  \n",
    "\n",
    "<img src='https://kishorepv.github.io/images/DSSM_layers.png' width=700>\n",
    "\n",
    "где $R(Q, D) = cosine(y_{Q}, y_{D}) = \\frac{y_{Q}^{T} y_{D}}{||y_{Q}|| \\cdot ||y_{D}||}$. \n",
    "\n",
    "При этом, считаем оценку семантической релевантности текущего документа и запроса как:\n",
    "\n",
    "$$P(D|Q) = \\frac{exp(\\gamma R(Q, D))}{\\sum_{D' \\in D}exp(\\gamma R(Q, D') }$$\n",
    "\n",
    "Проблема со знаменателем давно известна, поэтому минимизировать будем следующую функцию:\n",
    "\n",
    "$$L (Λ) = - log \\prod_{(Q, D^{+})} P(D^{+}|Q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f57f54",
   "metadata": {},
   "source": [
    "Слева - уже знакомая нам постановка задачи для матричной факторизации. Справа - как можем модифицировать DSSM под решение задачи рекомендаций и с обогащением признаками.\n",
    "\n",
    "<img src='images/dot_prod.png' width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a72e0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "References: \n",
    "    paper: (CIKM'2013) Learning Deep Structured Semantic Models for Web Search using Clickthrough Data\n",
    "    url: https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf\n",
    "    code: https://github.com/bbruceyuan/DeepMatch-Torch/blob/main/deepmatch_torch/models/dssm.py\n",
    "\"\"\"\n",
    "\n",
    "class DSSM(torch.nn.Module):\n",
    "    \"\"\"Deep Structured Semantic Model\n",
    "    Args:\n",
    "        user_features (list[Feature Class]): training by the user tower module.\n",
    "        item_features (list[Feature Class]): training by the item tower module.\n",
    "        temperature (float): temperature factor for similarity score, default to 1.0.\n",
    "        user_params (dict): the params of the User Tower module, \n",
    "        keys include:`{\"dims\":list, \"activation\":str, \"dropout\":float, \"output_layer\":bool`}.\n",
    "        item_params (dict): the params of the Item Tower module, keys include:`{\"dims\":list, \"activation\":str, \"dropout\":float, \"output_layer\":bool`}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, user_features, item_features, user_params, item_params, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.user_features = user_features\n",
    "        self.item_features = item_features\n",
    "        self.temperature = temperature\n",
    "        self.user_dims = sum([f.embed_dim for f in user_features])\n",
    "        self.item_dims = sum([f.embed_dim for f in item_features])\n",
    "\n",
    "        self.embedding = EmbeddingLayer(user_features + item_features)\n",
    "        self.user_mlp = MLP(self.user_dims, output_layer=False, **user_params)\n",
    "        self.item_mlp = MLP(self.item_dims, output_layer=False, **item_params)\n",
    "        self.mode = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_embedding = self.user_tower(x)\n",
    "        item_embedding = self.item_tower(x)\n",
    "        if self.mode == \"user\":\n",
    "            return user_embedding\n",
    "        if self.mode == \"item\":\n",
    "            return item_embedding\n",
    "        y = # YOUR ONE-LINE CODE HERE\n",
    "        return torch.sigmoid(y)\n",
    "\n",
    "    def item_tower(self, x):\n",
    "        if self.mode == \"user\":\n",
    "            return None\n",
    "        # Какая тут размерность? \n",
    "        input_item = self.embedding(x, self.item_features, squeeze_dim=True)\n",
    "        item_embedding = self.item_mlp(input_item)\n",
    "        item_embedding = F.normalize(item_embedding, p=2, dim=1)\n",
    "        return item_embedding\n",
    "    \n",
    "    def user_tower(self, x):\n",
    "        if self.mode == \"item\":\n",
    "            return None\n",
    "        input_user = self.embedding(x, self.user_features, squeeze_dim=True)\n",
    "        user_embedding = self.user_mlp(input_user)\n",
    "        user_embedding = F.normalize(user_embedding, p=2, dim=1)\n",
    "        return user_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb9bed",
   "metadata": {},
   "source": [
    "* Про температуру\n",
    "\n",
    "Обычно, при более высокой температуре вероятности становятся более равномерными, что означает, что модель более \"размазывает\" вероятности между различными классами или вариантами ответов. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202c359",
   "metadata": {},
   "source": [
    "Кстати, recap, как называется архитектура с картинки ниже?\n",
    "\n",
    "Сравните ее еще раз с архитектурами DSSM и CF (ч.с. MF) и не путайте =)\n",
    "\n",
    "<img src='images/NCF.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80cefa",
   "metadata": {},
   "source": [
    "Есть много имплементаций DSSM, можно посмотреть, например, в библиотеке [RecBole](https://github.com/RUCAIBox/RecBole/blob/4b6c6fef9b2f21326876f81e5d76631b280b0909/recbole/model/context_aware_recommender/dssm.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2450865",
   "metadata": {},
   "source": [
    "<img src='images/dot_prod.png' width=600>\n",
    "\n",
    "Как будем оптимизировать для RecSys? Лоссы можно выбирать разные. Самые простые из них:\n",
    "\n",
    "* **Кросс-энтропия**\n",
    "\n",
    "$\\hat p_{ui} = \\sigma (R(u, i)) = \\sigma(y_{u}^T y_{i})$ - вероятность, что юзер совершит действие с айтемом.\n",
    "\n",
    "Тогда функция потерь:\n",
    "\n",
    "$L = - \\sum_{u, i} (r_{ui}\\cdot log \\hat p_{ui} + (1 - r_{ui}) \\cdot log(1 - \\hat p_{ui}))$\n",
    "\n",
    "* **Triplet loss**\n",
    "\n",
    "Рассматриваем тройки из пользователя, положительного примера для него и отрицательного - $R(u, i_{+}, i_{-})$\n",
    "\n",
    "$L _{margin}(u,i,j)=max(0, margin + sim(u,i) − sim(u,j))$\n",
    "\n",
    "То есть, пытаемся увеличивать разницу, задавая некоторый порог (отступ) альфа. \n",
    "\n",
    "Для pairwise лоссов: \n",
    "\n",
    "$L _{BPR}(u,i,j)= - log(\\sigma(sim(u,i) − sim(u,j))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc67787",
   "metadata": {},
   "source": [
    "* Давайте тут остановимся и подумаем, какие варианты negative sampling при оптимизации лосса тут можно использвать?\n",
    "\n",
    "* Какие признаки можно использовать по users, items в качестве input данных? \n",
    "\n",
    "* Какие преимущества и недостатки у DSSM модели?\n",
    "\n",
    "\n",
    "| Метод | Достоинства  |  Недостатки | \n",
    "|---|---|---|\n",
    "| DSSM| Может выявить нелинейные закономерности  |  Дополнительная работа с признаками и оценкой их вклада|  \n",
    "|\n",
    "|| Real-time inference на проде | Необходимость подбора параметров и тюнинга гиперпараметров |  \n",
    "|\n",
    "|| Возможность добавлять признаки разного типа| Уже вряд ли можно назвать SOTA по качеству ранжирования | \n",
    "|\n",
    "|| Подходит как мета-алгоритм для каскадной модели| |  \n",
    "|\n",
    "|| User и item эмбеддинги в едином векторном пространстве| |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815SCcTVlFg-",
   "metadata": {
    "id": "815SCcTVlFg-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### MovieLens-1M\n",
    "\n",
    "Источник: https://grouplens.org/datasets/movielens/1m/ с данными в трех файлах: users.dat, ratings.dat, movies.dat, которые мы объединились в один csv. \n",
    "Для быстроты вычислений на семинаре мы взяли первые 10к строк из датасета, но если будете воспроизводить у себя и захотите посмотреть на финальное качество модели, попробуйте поработать со всем датасетом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a28daa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>James and the Giant Peach (1996)</td>\n",
       "      <td>Animation|Children's|Musical</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>My Fair Lady (1964)</td>\n",
       "      <td>Musical|Romance</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  timestamp                                   title                        genres gender  age  occupation    zip\n",
       "0        1      1193       5  978300760  One Flew Over the Cuckoo's Nest (1975)                         Drama      F    1          10  48067\n",
       "1        1       661       3  978302109        James and the Giant Peach (1996)  Animation|Children's|Musical      F    1          10  48067\n",
       "2        1       914       3  978301968                     My Fair Lady (1964)               Musical|Romance      F    1          10  48067\n",
       "3        1      3408       4  978300275                  Erin Brockovich (2000)                         Drama      F    1          10  48067\n",
       "4        1      2355       5  978824291                    Bug's Life, A (1998)   Animation|Children's|Comedy      F    1          10  48067"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'ml-1m.csv'\n",
    "data = pd.read_csv(file_path, nrows=10000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed2db3",
   "metadata": {},
   "source": [
    "Препроцессинг - энкодинг, обработка колонки с жанрами (оставим только первый указанный), сделаем split по числу интеракций в тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c8706e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cat_id\"] = data[\"genres\"].apply(lambda x: x.split(\"|\")[0])\n",
    "user_col, item_col = \"user_id\", \"movie_id\"\n",
    "sparse_features = ['user_id', 'movie_id', 'gender', 'age', 'occupation', 'zip', \"cat_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "CXAO69hclFhC",
   "metadata": {
    "id": "CXAO69hclFhC",
    "outputId": "f68453c4-b1aa-4728-c929-1ec883e4ed6f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before encoding: \n",
      "       user_id  movie_id gender  age  occupation   zip  cat_id\n",
      "9995      180      1193      M   45          12  1603   Drama\n",
      "9996      180      3408      M   45          12  1603   Drama\n",
      "9997      180       608      M   45          12  1603   Crime\n",
      "9998      180      3068      M   45          12  1603   Drama\n",
      "9999      180      3578      M   45          12  1603  Action\n",
      "\n",
      "After encoding: \n",
      "       user_id  movie_id  gender  age  occupation  zip  cat_id\n",
      "9995       46       644       2    5          11    2       8\n",
      "9996       46      1881       2    5          11    2       8\n",
      "9997       46       365       2    5          11    2       6\n",
      "9998       46      1709       2    5          11    2       8\n",
      "9999       46      1983       2    5          11    2       1\n"
     ]
    }
   ],
   "source": [
    "save_dir = './saved/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "print(f'Before encoding: \\n {data[sparse_features].tail()}')\n",
    "\n",
    "feature_max_idx = {}\n",
    "for feature in sparse_features:\n",
    "    encoder = LabelEncoder()\n",
    "    data[feature] = encoder.fit_transform(data[feature]) + 1 # лучше энкодить не с 0, особенно в sequential NN\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "    if feature == user_col:\n",
    "        user_map = {encode_id + 1: raw_id for encode_id, raw_id in enumerate(encoder.classes_)}\n",
    "    if feature == item_col:\n",
    "        item_map = {encode_id + 1: raw_id for encode_id, raw_id in enumerate(encoder.classes_)}\n",
    "np.save(save_dir + \"raw_id_maps.npy\", (user_map, item_map))\n",
    "\n",
    "print(f'\\nAfter encoding: \\n {data[sparse_features].tail()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ofoMu3SKlFhD",
   "metadata": {
    "id": "ofoMu3SKlFhD",
    "outputId": "a4673ec1-74eb-43ab-edde-002a66901b64",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_cols = [\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "item_cols = ['movie_id', \"cat_id\"]\n",
    "user_profile = data[user_cols].drop_duplicates('user_id')\n",
    "item_profile = data[item_cols].drop_duplicates('movie_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd1610",
   "metadata": {},
   "source": [
    "Наиболее интересные тут параметры - `sample_method`, `mode`, `neg_ratio`, `min_item`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "XZ4jzoiulFhE",
   "metadata": {
    "id": "XZ4jzoiulFhE",
    "outputId": "ead082af-3809-4682-a7c2-769894bc2727",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate sequence features: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 262.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train: 59430, n_test: 45\n",
      "1 cold start users droped \n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = generate_seq_feature_match(data,\n",
    "                                               user_col,\n",
    "                                               item_col,\n",
    "                                               time_col=\"timestamp\",\n",
    "                                               item_attribute_cols=[],\n",
    "                                               sample_method=2,\n",
    "                                               mode=0,\n",
    "                                               neg_ratio=5,\n",
    "                                               min_item=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b031a",
   "metadata": {},
   "source": [
    "Важно помнить про `max sequence length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8733bca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': array([34, 17, 44]),\n",
       " 'movie_id': array([ 516,  409, 2027]),\n",
       " 'hist_movie_id': array([[ 653,   38, 2038, 1060,  670,  690,  644,  674,  481, 1121, 1726,\n",
       "          647,  152,  700,  748,  720,  609,  668, 2065,  365,  897, 1570,\n",
       "          693,  485, 1739,  472,  774,  477,  793,  799,  663,  276, 1076,\n",
       "          649,  686,   63, 1065,    6,  729, 1618,  515, 1077,  688,  474,\n",
       "          656,  437,  708,  355,  597,  604],\n",
       "        [ 358, 1150,  220,  726,  119,  483,  359, 1155, 1162,  553,  558,\n",
       "          564,  336, 1151,  562,  385,  353, 1148,  559,  867,  561,  577,\n",
       "          141, 1536,  522,  935,  790,  595,  426,  470,  839,  560, 1111,\n",
       "          442, 1156, 1994,  970,   37,  974, 1997,  703,  793, 1315, 2033,\n",
       "          722, 1095,  757, 1936,  433, 1087],\n",
       "        [1894,  557,  945, 1802, 1372,  600, 1362,  146, 1257,  734, 1446,\n",
       "         1203, 1136,   33, 2056, 1746, 1752,  570, 1285,  682,  678,    1,\n",
       "         1671, 1742,  799,  722, 1309, 1342, 1873,  484, 1306, 1999,  729,\n",
       "         1150, 1138,  587, 1619,  558,  823,  751, 1516,  584, 1015,  843,\n",
       "         1341, 1616,  848, 1096, 1171,  909]]),\n",
       " 'histlen_movie_id': array([109, 396, 176]),\n",
       " 'label': array([0, 1, 0]),\n",
       " 'gender': array([2, 2, 1]),\n",
       " 'age': array([4, 3, 3]),\n",
       " 'occupation': array([8, 1, 3]),\n",
       " 'zip': array([29, 42, 39]),\n",
       " 'cat_id': array([8, 5, 8])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = gen_model_input(df_train, user_profile, user_col, item_profile, item_col, seq_max_len=50)\n",
    "x_test = gen_model_input(df_test, user_profile, user_col, item_profile, item_col, seq_max_len=50)\n",
    "y_train = x_train[\"label\"]\n",
    "y_test = x_test[\"label\"]\n",
    "\n",
    "{k: v[:3] for k, v in x_train.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "scu7A3gflFhF",
   "metadata": {
    "id": "scu7A3gflFhF",
    "outputId": "cddb3abb-f777-4bb1-a964-b3aa14208fa1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_features = [\n",
    "    SparseFeature(feature_name, vocab_size=feature_max_idx[feature_name], embed_dim=16) for feature_name in user_cols\n",
    "]\n",
    "\n",
    "user_features += [\n",
    "    SequenceFeature(\"hist_movie_id\",\n",
    "                    vocab_size=feature_max_idx[\"movie_id\"],\n",
    "                    embed_dim=16,\n",
    "                    pooling=\"mean\",\n",
    "                    shared_with=\"movie_id\")\n",
    "]\n",
    "\n",
    "item_features = [\n",
    "    SparseFeature(feature_name, vocab_size=feature_max_idx[feature_name], embed_dim=16) for feature_name in item_cols\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec748585",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_item = df_to_dict(item_profile)\n",
    "test_user = x_test\n",
    "data_generator = MatchDataGenerator(x=x_train, y=y_train)\n",
    "train_dl, test_dl, item_dl = data_generator.generate_dataloader(test_user, all_item, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb791e6",
   "metadata": {
    "id": "4fb791e6",
    "outputId": "557de275-42df-451d-94b1-15990999f80d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 465/465 [00:22<00:00, 20.86it/s, loss=0.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 465/465 [00:21<00:00, 21.79it/s, loss=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:  14%|█████████████▌                                                                                      | 63/465 [00:04<00:28, 14.19it/s, loss=0.47]"
     ]
    }
   ],
   "source": [
    "model = DSSM(user_features,\n",
    "             item_features,\n",
    "             temperature=0.02,\n",
    "             user_params={\n",
    "                 \"dims\": [256, 128, 64],\n",
    "                 \"activation\": 'prelu',\n",
    "             },\n",
    "             item_params={\n",
    "                 \"dims\": [256, 128, 64],\n",
    "                 \"activation\": 'prelu',\n",
    "             })\n",
    "\n",
    "trainer = MatchTrainer(model,\n",
    "                       mode=0,\n",
    "                       optimizer_params={\n",
    "                           \"lr\": 1e-2,\n",
    "                           \"weight_decay\": 1e-5\n",
    "                       },\n",
    "                       n_epoch=3,\n",
    "                       device='cpu',\n",
    "                       model_path=save_dir)\n",
    "\n",
    "\n",
    "trainer.fit(train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffa61a",
   "metadata": {},
   "source": [
    "Как вы думаете, зачем нам понадобился ANN алгоритм? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866e820",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def match_evaluation(user_embedding, item_embedding, test_user, all_item, user_col='user_id', item_col='movie_id',\n",
    "                     raw_id_maps=\"./raw_id_maps.npy\", topk=10):\n",
    "    \n",
    "    # Fit Annoy tree on item embeddings\n",
    "    annoy = Annoy(n_trees=10)\n",
    "    annoy.fit(item_embedding)\n",
    "\n",
    "    # For each user get top-k similar items\n",
    "    user_map, item_map = np.load(raw_id_maps, allow_pickle=True)\n",
    "    match_res = collections.defaultdict(dict)\n",
    "    for user_id, user_emb in zip(test_user[user_col], user_embedding):\n",
    "        items_idx, items_scores = annoy.query(v=user_emb, n=topk)\n",
    "        match_res[user_map[user_id]] = np.vectorize(item_map.get)(all_item[item_col][items_idx])\n",
    "\n",
    "    # Get ground truth\n",
    "    data = pd.DataFrame({user_col: test_user[user_col], item_col: test_user[item_col]})\n",
    "    data[user_col] = data[user_col].map(user_map)\n",
    "    data[item_col] = data[item_col].map(item_map)\n",
    "    user_pos_item = data.groupby(user_col).agg(list).reset_index()\n",
    "    ground_truth = dict(zip(user_pos_item[user_col], user_pos_item[item_col]))\n",
    "\n",
    "    # Compute top-k metrics\n",
    "    out = topk_metrics(y_true=ground_truth, y_pred=match_res, topKs=[topk])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CKfpMh0vlFhG",
   "metadata": {
    "id": "CKfpMh0vlFhG",
    "outputId": "a722cd9d-1909-423b-a7d1-52e84a6c10e3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user_embedding = trainer.inference_embedding(model=model, mode=\"user\", data_loader=test_dl, model_path=save_dir)\n",
    "item_embedding = trainer.inference_embedding(model=model, mode=\"item\", data_loader=item_dl, model_path=save_dir)\n",
    "match_evaluation(user_embedding, item_embedding, test_user, all_item, topk=100, raw_id_maps=save_dir + \"raw_id_maps.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yA0ORyDTlFhH",
   "metadata": {
    "id": "yA0ORyDTlFhH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### YouTubeDNN\n",
    "\n",
    "Covington, P., Adams, J. and Sargin, E., 2016, September. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems (pp. 191-198).\n",
    "\n",
    "https://dl.acm.org/doi/pdf/10.1145/2959100.2959190\n",
    "\n",
    "<img src='images/YoutubeDNN.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc683e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoutubeDNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The match model mentioned in `Deep Neural Networks for YouTube Recommendations` paper.\n",
    "    It's a DSSM match model trained by global softmax loss on list-wise samples. \n",
    "    In origin paper, item dnn tower is missing.\n",
    "    Args:\n",
    "        user_features (list[Feature Class]): training by the user tower module.\n",
    "        item_features (list[Feature Class]): training by the embedding table, it's the item id feature.\n",
    "        neg_item_feature (list[Feature Class]): training by the embedding table, it's the negative items id feature.\n",
    "        user_params (dict): the params of the User Tower module, \n",
    "        keys include:`{\"dims\":list, \"activation\":str, \"dropout\":float, \"output_layer\":bool`}.\n",
    "        temperature (float): temperature factor for similarity score, default to 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, user_features, item_features, neg_item_feature, user_params, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.user_features = user_features\n",
    "        self.item_features = item_features\n",
    "        self.neg_item_feature = neg_item_feature\n",
    "        self.temperature = temperature\n",
    "        self.user_dims = sum([fea.embed_dim for fea in user_features])\n",
    "        self.embedding = EmbeddingLayer(user_features + item_features)\n",
    "        self.user_mlp = MLP(self.user_dims, output_layer=False, **user_params)\n",
    "        self.mode = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        user_embedding = self.user_tower(x)\n",
    "        item_embedding = self.item_tower(x)\n",
    "        if self.mode == \"user\":\n",
    "            return user_embedding\n",
    "        if self.mode == \"item\":\n",
    "            return item_embedding\n",
    "\n",
    "        y = torch.mul(user_embedding, item_embedding).sum(dim=2)\n",
    "        y = y / self.temperature\n",
    "        return y\n",
    "\n",
    "    def user_tower(self, x):\n",
    "        if self.mode == \"item\":\n",
    "            return None\n",
    "        # [batch_size, num_features * deep_dims]\n",
    "        input_user = self.embedding(x, self.user_features, squeeze_dim=True)\n",
    "        # [batch_size, 1, embed_dim]\n",
    "        user_embedding = self.user_mlp(input_user).unsqueeze(1)\n",
    "        user_embedding = F.normalize(user_embedding, p=2, dim=2)\n",
    "        if self.mode == \"user\":\n",
    "            return user_embedding.squeeze(1)\n",
    "        return user_embedding\n",
    "\n",
    "    def item_tower(self, x):\n",
    "        if self.mode == \"user\":\n",
    "            return None\n",
    "        #[batch_size, 1, embed_dim]\n",
    "        pos_embedding = self.embedding(x, self.item_features, squeeze_dim=False)\n",
    "        pos_embedding = F.normalize(pos_embedding, p=2, dim=2)\n",
    "        # inference embedding mode\n",
    "        if self.mode == \"item\":\n",
    "            # [batch_size, embed_dim]\n",
    "            return pos_embedding.squeeze(1)\n",
    "        #[batch_size, n_neg_items, embed_dim]\n",
    "        neg_embeddings = self.embedding(x, self.neg_item_feature,\n",
    "                                        squeeze_dim=False).squeeze(1)\n",
    "        neg_embeddings = F.normalize(neg_embeddings, p=2, dim=2)\n",
    "        # [batch_size, 1 + n_neg_items, embed_dim]\n",
    "        return torch.cat((pos_embedding, neg_embeddings), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = generate_seq_feature_match(data,\n",
    "                                               user_col,\n",
    "                                               item_col,\n",
    "                                               time_col=\"timestamp\",\n",
    "                                               item_attribute_cols=[],\n",
    "                                               sample_method=1,\n",
    "                                               mode=2,\n",
    "                                               neg_ratio=3,\n",
    "                                               min_item=0)\n",
    "x_train = gen_model_input(df_train, user_profile, user_col, item_profile, item_col, seq_max_len=50)\n",
    "y_train = np.array([0] * df_train.shape[0])\n",
    "x_test = gen_model_input(df_test, user_profile, user_col, item_profile, item_col, seq_max_len=50)\n",
    "\n",
    "user_cols = ['user_id', 'gender', 'age', 'occupation', 'zip']\n",
    "\n",
    "user_features = [SparseFeature(name, vocab_size=feature_max_idx[name], embed_dim=16) for name in user_cols]\n",
    "user_features += [\n",
    "    SequenceFeature(\"hist_movie_id\",\n",
    "                    vocab_size=feature_max_idx[\"movie_id\"],\n",
    "                    embed_dim=16,\n",
    "                    pooling=\"mean\",\n",
    "                    shared_with=\"movie_id\")\n",
    "]\n",
    "\n",
    "item_features = [SparseFeature('movie_id', vocab_size=feature_max_idx['movie_id'], embed_dim=16)]\n",
    "neg_item_feature = [\n",
    "    SequenceFeature('neg_items',\n",
    "                    vocab_size=feature_max_idx['movie_id'],\n",
    "                    embed_dim=16,\n",
    "                    pooling=\"concat\",\n",
    "                    shared_with=\"movie_id\")\n",
    "]\n",
    "\n",
    "all_item = df_to_dict(item_profile)\n",
    "test_user = x_test\n",
    "\n",
    "dg = MatchDataGenerator(x=x_train, y=y_train)\n",
    "train_dl, test_dl, item_dl = dg.generate_dataloader(test_user, all_item, batch_size=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oM0gkV2KlFhH",
   "metadata": {
    "id": "oM0gkV2KlFhH",
    "outputId": "d5077600-b9ca-4cf4-a041-7d1e04b799da",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = YoutubeDNN(user_features, item_features, neg_item_feature, \n",
    "                   user_params={\"dims\": [128, 64, 16]}, temperature=0.02)\n",
    "\n",
    "trainer = MatchTrainer(model,\n",
    "                       mode=2,\n",
    "                       optimizer_params={\n",
    "                           \"lr\": 1e-2,\n",
    "                           \"weight_decay\": 1e-5\n",
    "                       },\n",
    "                       n_epoch=1,\n",
    "                       device='cpu',\n",
    "                       model_path=save_dir)\n",
    "\n",
    "trainer.fit(train_dl)\n",
    "\n",
    "print(\"inference embedding\")\n",
    "user_embedding = trainer.inference_embedding(model=model, mode=\"user\", data_loader=test_dl, model_path=save_dir)\n",
    "item_embedding = trainer.inference_embedding(model=model, mode=\"item\", data_loader=item_dl, model_path=save_dir)\n",
    "match_evaluation(user_embedding, item_embedding, test_user, all_item, topk=100, raw_id_maps=\"./saved/raw_id_maps.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5c403",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### LightFM. \n",
    "\n",
    "Kula, M., 2015. Metadata embeddings for user and item cold-start recommendations. arXiv preprint arXiv:1507.08439.\n",
    "\n",
    "http://ceur-ws.org/Vol-1448/paper4.pdf?ref=https://githubhelp.com\n",
    "\n",
    "https://github.com/lyst/lightfm\n",
    "\n",
    "Нейросетевые модели - не всегда самая лучшая опция, в частности, есть довольно сильная модель LightFM, которая обобщает коллабративную фильтрацию для случая с добавлением признаков. \n",
    "\n",
    "\n",
    "* Это гибридный подход коллаборативной фильтрации и контентной модели, которая предсталвяет эмбеддинги пользователей и эмбеддинги объектов как линейные комбинации из обученных векторов известных признаков - т.е. суммы новых латентных признаков. При этом, это позволяет обучать модель как в режиме без признаков, так и с ними, решая проблему холодного старта (т.к. по новым пользователям и объектам можно использовать их признаки) и проблему слишком разреженных данных (high sparsity problem). Таким образом, LightFM умеет хорошо работает как с плотными, так и с разреженными данными, и, как бонус, кодировать в эмбеддингах признаков семантическую информацию по аналогии с подходами для получения эмбеддингов слов (например, w2v).\n",
    "\n",
    "\n",
    "*  Формализация.  <br> $U$ - множество пользователей, <br> $I$ - множество объектов, <br> $F^{U}$ - множество признаков пользователей, <br> $F^{I}$- множество признаков объектов. <br>\n",
    "Все пары $(u, i) \\in U × I$ - это объединение всех положительных $S^{+}$ и отрицательных $S^{-}$ интеракций. \n",
    "\n",
    "Каждый пользователь описан набором заранее известных признаков (мета данных) $f_u \\subset F^U$, то же самое для объектов  $f_i \\subset F^I$.\n",
    "\n",
    "Латентное представление пользователя представлено суммой его латентных векторов признаков: \n",
    "\n",
    "$$q_u = \\sum_{j \\in f_u} e^U_j$$\n",
    "\n",
    "Аналогично для объектов: $$p_u = \\sum_{j \\in f_i} e^I_j$$\n",
    "\n",
    "Так же, по пользователю и объекту есть смещения (bias): \n",
    "\n",
    "$$b_u = \\sum_{j \\in f_u} b^U_j$$\n",
    "\n",
    "$$b_i = \\sum_{j \\in f_i} b^I_j$$\n",
    "\n",
    "Предсказание из модели будет получать через скалярное произведение эмбедингов пользователя и объекта. \n",
    "\n",
    "$$\\hat r_{ui} = f (q_u \\cdot p_i + b_u + b_i)$$\n",
    "\n",
    "Функция f() может быть разной, автор статьи выбрал сигмоиду, поскольку использовал бинарные данные.\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + exp(-x)}$$\n",
    "\n",
    "Задача оптимизации будет сформулирована как максимизация правдоподобия (данных при параметрах), с обучением модели с помощью стохастического градиентного спуска.\n",
    "\n",
    "$$L(e^U, e^I, b^U, b^I) = \\prod_{(u, i) \\in S^+} \\hat r_{ui} \\cdot  \\prod_{(u, i) \\in S^-} (1 - \\hat r_{ui})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e9958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tuple_iterator(df: pd.DataFrame):\n",
    "    '''\n",
    "    :df: pd.DataFrame, interactions dataframe\n",
    "    returs iterator\n",
    "    '''\n",
    "    return zip(*df.values.T)\n",
    "\n",
    "def concat_last_to_list(t):\n",
    "    return (t[0], list(t[1:])[0])\n",
    "\n",
    "def df_to_tuple_list_iterator(df):\n",
    "    return map(concat_last_to_list, zip(*df.values.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f0a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv('ratings_small.csv')\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_metadata = pd.read_csv('movies_metadata.csv')\n",
    "movies_metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_metadata['id'] = movies_metadata['id'].astype(str)\n",
    "interactions['movieId'] = interactions['movieId'].astype(str)\n",
    "\n",
    "interactions_filtered = interactions.loc[interactions['movieId'].isin(movies_metadata['id'])]\n",
    "print(interactions.shape, interactions_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b013469",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "dataset.fit(interactions['userId'].unique(), interactions['movieId'].unique())\n",
    "lightfm_mapping = dataset.mapping()\n",
    "lightfm_mapping = {\n",
    "    'users_mapping': lightfm_mapping[0],\n",
    "    'user_features_mapping': lightfm_mapping[1],\n",
    "    'items_mapping': lightfm_mapping[2],\n",
    "    'item_features_mapping': lightfm_mapping[3],\n",
    "}\n",
    "print('user mapper length - ', len(lightfm_mapping['users_mapping']))\n",
    "print('user features mapper length - ', len(lightfm_mapping['user_features_mapping']))\n",
    "print('movies mapper length - ', len(lightfm_mapping['items_mapping']))\n",
    "print('Users movie features mapper length - ', len(lightfm_mapping['item_features_mapping']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightfm_mapping['users_inv_mapping'] = {v: k for k, v in lightfm_mapping['users_mapping'].items()}\n",
    "lightfm_mapping['items_inv_mapping'] = {v: k for k, v in lightfm_mapping['items_mapping'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2363137",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat, train_mat_weights = dataset.build_interactions(\n",
    "    df_to_tuple_iterator(interactions_filtered[['userId', 'movieId']])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f7476",
   "metadata": {},
   "source": [
    "Set LightFM params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2251eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_COMPONENTS = 64\n",
    "LEARNING_RATE = .04\n",
    "LOSS = 'bpr'\n",
    "MAX_SAMPLED = 5\n",
    "RANDOM_STATE = 42\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3be02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfm_model = LightFM(\n",
    "    no_components = NO_COMPONENTS,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    loss = LOSS,\n",
    "    max_sampled = MAX_SAMPLED,\n",
    "    random_state = RANDOM_STATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e4054",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm_notebook(range(EPOCHS), total = EPOCHS):\n",
    "    lfm_model.fit_partial(\n",
    "        train_mat, \n",
    "        num_threads = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbbd34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to make sanity-check\n",
    "top_N = 10\n",
    "user_id = interactions['userId'][1000]\n",
    "row_id = lightfm_mapping['users_mapping'][user_id]\n",
    "print(f'Rekko for user {user_id}, row number in matrix - {row_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc1394",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = list(lightfm_mapping['items_mapping'].values())\n",
    "len(all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7a16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lfm_model.predict(\n",
    "    row_id,\n",
    "    all_cols,\n",
    "    num_threads = 4)\n",
    "pred, pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cols = np.argpartition(pred, -np.arange(top_N))[-top_N:][::-1]\n",
    "top_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f23789",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[top_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4c3c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crate mapper for movieId and title names\n",
    "item_name_mapper = dict(zip(movies_metadata['id'], movies_metadata['original_title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.DataFrame({'col_id': top_cols})\n",
    "recs['movieId'] = recs['col_id'].map(lightfm_mapping['items_inv_mapping'].get).astype(str)\n",
    "recs['title'] = recs['movieId'].map(item_name_mapper)\n",
    "recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f53267",
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.DataFrame({'col_id': top_cols})\n",
    "recs['movieId'] = recs['col_id'].map(lightfm_mapping['items_inv_mapping'].get).astype(str)\n",
    "recs['title'] = recs['movieId'].map(item_name_mapper)\n",
    "recs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a4cb8",
   "metadata": {},
   "source": [
    "Источники:\n",
    "\n",
    "1. Исходная статья DSSM https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf \n",
    "2. RecBole https://github.com/RUCAIBox/RecBole\n",
    "3. https://github.com/datawhalechina/torch-rechub \n",
    "4. Статья LightFM http://ceur-ws.org/Vol-1448/paper4.pdf?ref=https://githubhelp.com\n",
    "5. Отличная **библиотека от автора статьи** c примерами запусков - https://github.com/lyst/lightfm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0161c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Torch-Rechub Tutorial：Matching.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "2f0699014af7f4c9080a159fe6ab9f0087a283cb8192b31d41a414a088fd29ff"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
